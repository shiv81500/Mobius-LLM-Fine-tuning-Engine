# ğŸš€ Mobius LLM Fine-Tuning Engine

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white)
![Java](https://img.shields.io/badge/Java-21+-orange?style=for-the-badge&logo=openjdk&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red?style=for-the-badge&logo=pytorch&logoColor=white)
![License](https://img.shields.io/badge/License-Apache-green?style=for-the-badge)

**A complete desktop application for fine-tuning Large Language Models on CPU/GPU with GGUF export for LM Studio**

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Model Guide](#-model-recommendations) â€¢ [Training Tips](#-training-tips)

</div>

---

## ğŸ¯ What is This?

Mobius is a **local LLM fine-tuning engine** that lets you:
- Fine-tune small language models on your own data
- Export to GGUF format for use in **LM Studio**, Ollama, or llama.cpp
- Train on **CPU** (no GPU required!) with optimized settings

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PyQt6 GUI      â”‚â”€â”€â”€â”€â–¶â”‚  Java Backend   â”‚â”€â”€â”€â”€â–¶â”‚  Python ML Core â”‚
â”‚  (Desktop App)  â”‚â—€â”€â”€â”€â”€â”‚  (Orchestrator) â”‚â—€â”€â”€â”€â”€â”‚  (Training)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ“ **Multi-Format Data** | Upload JSONL, CSV, or TXT training data |
| ğŸ¤– **Model Selection** | Choose from CPU-optimized models |
| âš™ï¸ **LoRA Fine-Tuning** | Memory-efficient training with LoRA |
| ğŸ“Š **Live Monitoring** | Real-time training logs and metrics |
| â¸ï¸ **Training Controls** | Pause, resume, or cancel training |
| ğŸ“¦ **GGUF Export** | Convert to GGUF for LM Studio |
| ğŸ’» **CPU Optimized** | Works on systems with 8GB RAM |

---

## ğŸ”§ Model Recommendations

### For CPU Systems (8GB RAM)

| Model | Parameters | RAM Usage | Best For | Training Time |
|-------|------------|-----------|----------|---------------|
| **Qwen2-0.5B-Instruct** â­ | 500M | ~1GB | Q&A, Instructions | Fast |
| **SmolLM-360M-Instruct** | 360M | ~500MB | Simple Q&A | Fastest |
| **TinyLlama-1.1B-Chat** | 1.1B | ~2GB | Conversations | Medium |
| **Phi-2** | 2.7B | ~4GB | Complex Tasks | Slow |

### âš ï¸ Models to AVOID on CPU

| Model | Why Avoid |
|-------|-----------|
| DistilGPT2 | Not instruction-tuned, gives gibberish for Q&A |
| GPT-2 | Completion only, doesn't follow instructions |
| Llama-3-8B | Too large, needs 16GB+ RAM |
| Mistral-7B | Too large, needs 14GB+ RAM |

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **Java 21+**
- **Maven**
- **8GB+ RAM**

### Installation

```powershell
# 1. Clone the repository
git clone https://github.com/Adil-Ijaz7/Mobius-LLM-Fine-tuning-Engine.git
cd Mobius-LLM-Fine-tuning-Engine

# 2. Create Python virtual environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# 3. Install Python dependencies
pip install -r ml_core/requirements.txt
pip install -r gui/requirements.txt

# 4. Build Java backend
mvn clean package
```

### Running

**Terminal 1 - Start Backend:**
```powershell
java -jar target/llm-trainer-backend-1.0.0.jar
```

**Terminal 2 - Start GUI:**
```powershell
cd gui
python main.py
```

---

## ğŸ“ Training Data Format

Your data should be in **JSONL format** with `instruction` and `response` fields:

```jsonl
{"instruction": "What is machine learning?", "response": "Machine learning is a subset of AI..."}
{"instruction": "Explain neural networks", "response": "Neural networks are computing systems..."}
{"instruction": "What is Python?", "response": "Python is a programming language..."}
```

### Recommended Dataset Size

| Dataset Size | Epochs | Expected Quality |
|--------------|--------|------------------|
| 17 examples | 15-20 | Basic memorization |
| 50-100 examples | 10-15 | Good learning |
| 200-500 examples | 5-10 | Excellent results |
| 1000+ examples | 3-5 | Production quality |

---

## âš™ï¸ Training Settings for CPU

### Recommended Configuration

| Setting | Value | Why |
|---------|-------|-----|
| **Model** | `Qwen/Qwen2-0.5B-Instruct` | Small, instruction-tuned |
| **Epochs** | 10-20 | Small datasets need more passes |
| **Batch Size** | 1 | Memory efficiency |
| **Grad Accumulation** | 8 | Simulates larger batches |
| **Learning Rate** | 2e-4 | Good for small models |
| **Max Length** | 256 | Covers most Q&A pairs |
| **LoRA Rank** | 8 | CPU-efficient |
| **LoRA Alpha** | 16 | Standard ratio |

### Command Line Training

```powershell
python ml_core/training_script.py `
  --job-id my-training `
  --dataset data.jsonl `
  --base-model "Qwen/Qwen2-0.5B-Instruct" `
  --output-dir ./output `
  --epochs 15 `
  --batch-size 1 `
  --grad-accum 8 `
  --learning-rate 2e-4 `
  --max-length 256 `
  --lora-rank 8 `
  --lora-alpha 16
```

---

## ğŸ“¦ Using in LM Studio

After training, export to GGUF:

1. **Merge LoRA adapters** with base model
2. **Convert to GGUF** using llama.cpp
3. **Load in LM Studio** with correct prompt template

### LM Studio Prompt Template

Set these in LM Studio â†’ Settings â†’ Prompt Template:

| Field | Value |
|-------|-------|
| Before User | `### Instruction:\n` |
| After User | `\n\n` |
| Before Assistant | `### Response:\n` |
| After Assistant | `\n\n` |

---

## ğŸ” Troubleshooting

### Model Gives Gibberish Responses

**Cause:** Using completion model (DistilGPT2/GPT-2) for Q&A task

**Solution:** Use instruction-tuned model like `Qwen/Qwen2-0.5B-Instruct`

### Out of Memory Error

**Cause:** Model too large or max_length too high

**Solutions:**
- Use smaller model (SmolLM-360M)
- Reduce `--max-length` to 128
- Add `--stream` flag
- Close other applications

### Training Too Slow

**Cause:** CPU training is inherently slow

**Solutions:**
- Use smaller model (SmolLM vs TinyLlama)
- Reduce epochs for testing
- Use `--lora-rank 4` for faster training

---

## ğŸ“ Project Structure

```
Mobius-LLM-Fine-tuning-Engine/
â”œâ”€â”€ gui/                    # PyQt6 Desktop Application
â”‚   â”œâ”€â”€ main.py            # Entry point
â”‚   â”œâ”€â”€ main_window.py     # Main window UI
â”‚   â”œâ”€â”€ steps/             # Wizard step panels
â”‚   â””â”€â”€ api/               # Backend API client
â”œâ”€â”€ ml_core/               # Python ML Training
â”‚   â”œâ”€â”€ training_script.py # Main training script
â”‚   â”œâ”€â”€ data_loader.py     # Dataset loading
â”‚   â”œâ”€â”€ convert_to_gguf.py # GGUF conversion
â”‚   â””â”€â”€ cpu_models.txt     # Model recommendations
â”œâ”€â”€ src/main/java/         # Java Backend
â”‚   â””â”€â”€ com/llmtrainer/    # Backend services
â”œâ”€â”€ data/                  # Training data storage
â””â”€â”€ pom.xml               # Maven build config
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ‘¤ Author

**Adil Ijaz**
- GitHub: [@Adil-Ijaz7](https://github.com/Adil-Ijaz7)

---

<div align="center">

â­ **Star this repo if you find it helpful!** â­

</div>
